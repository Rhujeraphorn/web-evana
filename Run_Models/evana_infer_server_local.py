# evana_infer_server_local.py
# ‡πÄ‡∏ã‡∏¥‡∏£‡πå‡∏ü‡πÄ‡∏ß‡∏≠‡∏£‡πå inference ‡πÇ‡∏´‡∏°‡∏î‡∏ï‡∏±‡∏ß‡∏≠‡∏¢‡πà‡∏≤‡∏á: ‡πÉ‡∏ä‡πâ‡∏ä‡∏∏‡∏î‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ñ‡∏≤‡∏°-‡∏ï‡∏≠‡∏ö‡∏ó‡∏µ‡πà‡πÄ‡∏ï‡∏£‡∏µ‡∏¢‡∏°‡πÑ‡∏ß‡πâ ‡πÅ‡∏ó‡∏ô‡∏Å‡∏≤‡∏£‡∏¢‡∏¥‡∏á‡πÄ‡∏Ç‡πâ‡∏≤‡πÇ‡∏°‡πÄ‡∏î‡∏•‡∏à‡∏£‡∏¥‡∏á
# ‡πÇ‡∏Ñ‡πâ‡∏î‡πÇ‡∏´‡∏•‡∏î‡πÇ‡∏°‡πÄ‡∏î‡∏•‡πÄ‡∏î‡∏¥‡∏°‡πÄ‡∏Å‡πá‡∏ö‡πÑ‡∏ß‡πâ‡∏î‡πâ‡∏≤‡∏ô‡∏•‡πà‡∏≤‡∏á‡πÉ‡∏ô‡∏ï‡∏±‡∏ß‡πÅ‡∏õ‡∏£ LEGACY_MODEL_IMPLEMENTATION (‡∏Ñ‡∏≠‡∏°‡πÄ‡∏°‡∏ô‡∏ï‡πå/‡∏ã‡πà‡∏≠‡∏ô ‡πÑ‡∏°‡πà‡πÑ‡∏î‡πâ‡∏£‡∏±‡∏ô)

import json
from pathlib import Path
from typing import Dict

from fastapi import FastAPI
from pydantic import BaseModel

DATA_FILE = Path(__file__).parent / "sample_qa_dataset.json"

app = FastAPI(title="EVANA Local Chat Inference (Sample Dataset)")


class ChatRequest(BaseModel):
    message: str


def normalize_text(text: str) -> str:
    """‡∏ï‡∏±‡∏î‡∏ä‡πà‡∏≠‡∏á‡∏ß‡πà‡∏≤‡∏á‡∏ã‡πâ‡∏≥ ‡πÜ ‡πÅ‡∏•‡∏∞‡∏õ‡∏£‡∏±‡∏ö‡πÄ‡∏õ‡πá‡∏ô lower-case ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏à‡∏±‡∏ö‡∏Ñ‡∏π‡πà‡∏Ñ‡∏≥‡∏ñ‡∏≤‡∏°‡∏ó‡∏µ‡πà‡πÄ‡∏ï‡∏£‡∏µ‡∏¢‡∏°‡πÑ‡∏ß‡πâ"""
    return " ".join(text.strip().split()).casefold()


def load_qa_lookup(path: Path) -> Dict[str, str]:
    """‡πÇ‡∏´‡∏•‡∏î‡∏ä‡∏∏‡∏î‡∏ñ‡∏≤‡∏°-‡∏ï‡∏≠‡∏ö‡∏ï‡∏±‡∏ß‡∏≠‡∏¢‡πà‡∏≤‡∏á‡πÄ‡∏õ‡πá‡∏ô dict ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÉ‡∏´‡πâ‡∏ï‡∏≠‡∏ö‡∏Å‡∏•‡∏±‡∏ö‡πÄ‡∏£‡πá‡∏ß ‡πÜ"""
    if not path.exists():
        print(f"[EVANA DEMO] ‡πÑ‡∏°‡πà‡∏û‡∏ö‡πÑ‡∏ü‡∏•‡πå‡∏ä‡∏∏‡∏î‡∏Ñ‡∏≥‡∏ñ‡∏≤‡∏°‡∏ï‡∏±‡∏ß‡∏≠‡∏¢‡πà‡∏≤‡∏á: {path}")
        return {}

    try:
        with path.open("r", encoding="utf-8") as f:
            items = json.load(f)
    except Exception as exc:
        print(f"[EVANA DEMO] ‡πÇ‡∏´‡∏•‡∏î‡πÑ‡∏ü‡∏•‡πå‡∏ï‡∏±‡∏ß‡∏≠‡∏¢‡πà‡∏≤‡∏á‡πÑ‡∏°‡πà‡∏™‡∏≥‡πÄ‡∏£‡πá‡∏à: {exc}")
        return {}

    lookup: Dict[str, str] = {}
    for item in items:
        question = normalize_text(str(item.get("question", "")))
        answer = item.get("answer", "").strip()
        if question and answer:
            lookup[question] = answer

    print(f"[EVANA DEMO] ‡πÇ‡∏´‡∏•‡∏î‡∏ä‡∏∏‡∏î‡∏Ñ‡∏≥‡∏ñ‡∏≤‡∏°‡∏ï‡∏±‡∏ß‡∏≠‡∏¢‡πà‡∏≤‡∏á‡πÅ‡∏•‡πâ‡∏ß {len(lookup)} ‡∏£‡∏≤‡∏¢‡∏Å‡∏≤‡∏£‡∏à‡∏≤‡∏Å {path.name}")
    return lookup


QA_LOOKUP = load_qa_lookup(DATA_FILE)
DEMO_HINT = (
    '‡∏•‡∏≠‡∏á‡∏ñ‡∏≤‡∏°‡∏ß‡πà‡∏≤ "‡πÄ‡∏•‡πà‡∏≤‡πÄ‡∏™‡πâ‡∏ô‡∏ó‡∏≤‡∏á‡∏à‡∏≤‡∏Å‡∏ï‡∏±‡∏ß‡πÄ‡∏°‡∏∑‡∏≠‡∏á‡∏•‡∏≥‡∏õ‡∏≤‡∏á‡πÑ‡∏õ‡∏¢‡∏±‡∏á‡∏ß‡∏±‡∏î‡∏û‡∏£‡∏∞‡∏ò‡∏≤‡∏ï‡∏∏‡∏•‡∏≥‡∏õ‡∏≤‡∏á‡∏´‡∏•‡∏ß‡∏á‡πÉ‡∏´‡πâ‡∏´‡∏ô‡πà‡∏≠‡∏¢" '
    "‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏î‡∏π‡∏ï‡∏±‡∏ß‡∏≠‡∏¢‡πà‡∏≤‡∏á‡∏Ñ‡∏≥‡∏ï‡∏≠‡∏ö‡∏ó‡∏µ‡πà‡πÄ‡∏ï‡∏£‡∏µ‡∏¢‡∏°‡πÑ‡∏ß‡πâ"
)


@app.post("/chat")
def chat_endpoint(req: ChatRequest):
    normalized = normalize_text(req.message)
    answer = QA_LOOKUP.get(normalized)

    if answer:
        return {"reply": answer}

    fallback = (
        "‡∏ô‡∏µ‡πà‡∏Ñ‡∏∑‡∏≠ EVANA Chatbot Demo ‡∏Å‡∏£‡∏∏‡∏ì‡∏≤‡∏ñ‡∏≤‡∏°‡∏Ñ‡∏≥‡∏ñ‡∏≤‡∏°‡∏ï‡∏≤‡∏°‡∏ï‡∏±‡∏ß‡∏≠‡∏¢‡πà‡∏≤‡∏á‡∏ó‡∏µ‡πà‡∏°‡∏µ‡∏≠‡∏¢‡∏π‡πà‡πÉ‡∏ô‡πÑ‡∏ü‡∏•‡πå sample_qa_dataset.json "
        "‡∏ï‡∏≠‡∏ô‡∏ô‡∏µ‡πâ‡∏¢‡∏±‡∏á‡πÑ‡∏°‡πà‡∏°‡∏µ‡∏Ñ‡∏≥‡∏ï‡∏≠‡∏ö‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏Ñ‡∏≥‡∏ñ‡∏≤‡∏°‡∏ô‡∏µ‡πâ\n"
        f"{DEMO_HINT}"
    )
    return {"reply": fallback}


# -----------------------------------------------------------------------------
# Legacy model-based server preserved for reference (disabled in demo mode)
LEGACY_MODEL_IMPLEMENTATION = r"""
# evana_infer_server_local.py
# ‡πÄ‡∏ã‡∏¥‡∏£‡πå‡∏ü‡πÄ‡∏ß‡∏≠‡∏£‡πå inference ‡∏ó‡∏µ‡πà‡∏£‡∏±‡∏ô‡∏ö‡∏ô‡πÄ‡∏Ñ‡∏£‡∏∑‡πà‡∏≠‡∏á
# ‡πÇ‡∏´‡∏•‡∏î base Mistral-Nemo-Instruct-2407 ‡πÅ‡∏ö‡∏ö 4-bit + LoRA general 4 ‡∏à‡∏±‡∏á‡∏´‡∏ß‡∏±‡∏î ‡πÅ‡∏•‡πâ‡∏ß‡πÄ‡∏õ‡∏¥‡∏î API /chat

from fastapi import FastAPI
from pydantic import BaseModel
from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig
from peft import PeftModel
import torch
import sys
import time
import os

# ====== PATH ‡πÇ‡∏°‡πÄ‡∏î‡∏•‡∏ö‡∏ô‡πÄ‡∏Ñ‡∏£‡∏∑‡πà‡∏≠‡∏á ======
DEFAULT_BASE_MODEL_DIR = r"C:\Users\Nervously\evana\models\mistral-nemo-instruct-2407"
DEFAULT_ADAPTER_DIR    = r"C:\Users\Nervously\evana\models\evana-mistral-sft-general-20251113-0626"

BASE_MODEL_DIR = os.getenv("BASE_MODEL_DIR", DEFAULT_BASE_MODEL_DIR)
ADAPTER_DIR    = os.getenv("ADAPTER_DIR", DEFAULT_ADAPTER_DIR)

if not os.path.exists(BASE_MODEL_DIR):
    print(f"‚ùå ‡πÑ‡∏°‡πà‡∏û‡∏ö BASE_MODEL_DIR: {BASE_MODEL_DIR}")
    sys.exit(1)

if not os.path.exists(ADAPTER_DIR):
    print(f"‚ùå ‡πÑ‡∏°‡πà‡∏û‡∏ö ADAPTER_DIR: {ADAPTER_DIR}")
    sys.exit(1)

app = FastAPI(title="EVANA Local Chat Inference")

class ChatRequest(BaseModel):
    message: str

# ====== ‡πÄ‡∏ä‡πá‡∏Å GPU ‡∏Å‡πà‡∏≠‡∏ô‡πÄ‡∏•‡∏¢ ‡∏Å‡∏±‡∏ô OOM ======
if not torch.cuda.is_available():
    print("‚ùå ‡πÑ‡∏°‡πà‡∏û‡∏ö GPU ‡∏´‡∏£‡∏∑‡∏≠ PyTorch ‡πÑ‡∏°‡πà‡∏£‡∏≠‡∏á‡∏£‡∏±‡∏ö CUDA")
    print("‡πÇ‡∏°‡πÄ‡∏î‡∏• Mistral-Nemo-Instruct-2407 (8B) ‡πÉ‡∏´‡∏ç‡πà‡πÄ‡∏Å‡∏¥‡∏ô‡∏Å‡∏ß‡πà‡∏≤‡∏à‡∏∞‡∏£‡∏±‡∏ô‡∏ö‡∏ô CPU 32GB ‡πÑ‡∏î‡πâ‡∏≠‡∏¢‡πà‡∏≤‡∏á‡∏õ‡∏•‡∏≠‡∏î‡∏†‡∏±‡∏¢")
    print("‡πÇ‡∏õ‡∏£‡∏î‡πÄ‡∏ä‡πá‡∏Å‡∏ß‡πà‡∏≤‡∏ï‡∏¥‡∏î‡∏ï‡∏±‡πâ‡∏á torch ‡πÄ‡∏ß‡∏≠‡∏£‡πå‡∏ä‡∏±‡∏ô CUDA ‡πÅ‡∏•‡πâ‡∏ß ‡πÅ‡∏•‡∏∞‡∏•‡∏≠‡∏á‡πÉ‡∏´‡∏°‡πà‡∏≠‡∏µ‡∏Å‡∏Ñ‡∏£‡∏±‡πâ‡∏á")
    sys.exit(1)

device = torch.device("cuda")
print(f"‚úÖ ‡πÉ‡∏ä‡πâ GPU: {torch.cuda.get_device_name(0)}")

print("üîπ Loading tokenizer...")
tokenizer = AutoTokenizer.from_pretrained(
    BASE_MODEL_DIR,
    local_files_only=True,
    trust_remote_code=True,
    fix_mistral_regex=True,   # ‡πÅ‡∏Å‡πâ issue regex ‡∏ï‡∏≤‡∏° warning
)

if tokenizer.pad_token_id is None:
    tokenizer.pad_token = tokenizer.eos_token

# ====== ‡∏ï‡∏±‡πâ‡∏á‡∏Ñ‡πà‡∏≤ 4-bit quantization ‡∏î‡πâ‡∏ß‡∏¢ bitsandbytes ======
bnb_config = BitsAndBytesConfig(
    load_in_4bit=True,
    bnb_4bit_use_double_quant=True,
    bnb_4bit_quant_type="nf4",
    bnb_4bit_compute_dtype=torch.float16,
)

print("üîπ Loading base model (Mistral-Nemo-Instruct-2407) ‡πÅ‡∏ö‡∏ö 4-bit ‡∏ó‡∏±‡πâ‡∏á‡∏Å‡πâ‡∏≠‡∏ô‡∏ö‡∏ô GPU ...")
base_model = AutoModelForCausalLM.from_pretrained(
    BASE_MODEL_DIR,
    quantization_config=bnb_config,
    device_map={"": 0},       # ‡∏ó‡∏±‡πâ‡∏á‡πÇ‡∏°‡πÄ‡∏î‡∏•‡∏•‡∏á GPU 0 ‡∏ï‡∏±‡∏ß‡πÄ‡∏î‡∏µ‡∏¢‡∏ß
    local_files_only=True,
)

print("üîπ Loading LoRA adapter (general 4 provinces)...")
# ‡∏ï‡∏£‡∏á‡∏ô‡∏µ‡πâ‡πÑ‡∏°‡πà‡∏™‡πà‡∏á device_map / offload_dir ‡∏≠‡∏∞‡πÑ‡∏£‡πÄ‡∏•‡∏¢ ‡∏õ‡∏•‡πà‡∏≠‡∏¢‡πÉ‡∏´‡πâ‡∏ï‡∏≤‡∏° base_model
model = PeftModel.from_pretrained(
    base_model,
    ADAPTER_DIR,
    local_files_only=True,
)
model.eval()

if model.config.pad_token_id is None:
    model.config.pad_token_id = tokenizer.pad_token_id

print("‚úÖ Model ready.")


def generate_answer(message: str) -> str:
    \"\"\"‡∏ü‡∏±‡∏á‡∏Å‡πå‡∏ä‡∏±‡∏ô‡∏¢‡∏¥‡∏á‡πÄ‡∏Ç‡πâ‡∏≤‡πÇ‡∏°‡πÄ‡∏î‡∏•‡πÅ‡∏•‡πâ‡∏ß‡∏î‡∏∂‡∏á‡πÄ‡∏â‡∏û‡∏≤‡∏∞‡∏Ñ‡∏≥‡∏ï‡∏≠‡∏ö‡∏Ç‡∏≠‡∏á EVANA ‡∏≠‡∏≠‡∏Å‡∏°‡∏≤\"\"\"

    system_prompt = (
        "‡∏Ñ‡∏∏‡∏ì‡∏Ñ‡∏∑‡∏≠ EVANA Chatbot ‡∏ú‡∏π‡πâ‡∏ä‡πà‡∏ß‡∏¢‡πÅ‡∏ô‡∏∞‡∏ô‡∏≥‡∏Å‡∏≤‡∏£‡∏ó‡πà‡∏≠‡∏á‡πÄ‡∏ó‡∏µ‡πà‡∏¢‡∏ß‡∏†‡∏≤‡∏Ñ‡πÄ‡∏´‡∏ô‡∏∑‡∏≠‡∏Ç‡∏≠‡∏á‡πÑ‡∏ó‡∏¢‡∏î‡πâ‡∏ß‡∏¢‡∏£‡∏ñ‡∏¢‡∏ô‡∏ï‡πå‡πÑ‡∏ü‡∏ü‡πâ‡∏≤ "
        "‡∏ï‡∏≠‡∏ö‡πÅ‡∏ö‡∏ö‡∏Å‡∏£‡∏∞‡∏ä‡∏±‡∏ö ‡∏≠‡πà‡∏≤‡∏ô‡∏á‡πà‡∏≤‡∏¢ ‡πÄ‡∏õ‡πá‡∏ô‡∏Å‡∏±‡∏ô‡πÄ‡∏≠‡∏á ‡πÅ‡∏•‡∏∞‡∏≠‡∏¥‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏™‡∏ñ‡∏≤‡∏ô‡∏ó‡∏µ‡πà‡∏à‡∏£‡∏¥‡∏á‡πÉ‡∏´‡πâ‡∏°‡∏≤‡∏Å‡∏ó‡∏µ‡πà‡∏™‡∏∏‡∏î "
        "‡∏ñ‡πâ‡∏≤‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÑ‡∏°‡πà‡πÅ‡∏ô‡πà‡πÉ‡∏à‡πÉ‡∏´‡πâ‡πÄ‡∏ï‡∏∑‡∏≠‡∏ô‡∏ú‡∏π‡πâ‡πÉ‡∏ä‡πâ‡πÉ‡∏´‡πâ‡πÄ‡∏ä‡πá‡∏Å‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏•‡πà‡∏≤‡∏™‡∏∏‡∏î‡∏≠‡∏µ‡∏Å‡∏Ñ‡∏£‡∏±‡πâ‡∏á"
    )

    prompt = f"{system_prompt}\n\n‡∏ú‡∏π‡πâ‡πÉ‡∏ä‡πâ: {message}\nEVANA:"

    inputs = tokenizer(
        prompt,
        return_tensors="pt",
    ).to(device)

    with torch.inference_mode():
        output_ids = model.generate(
            **inputs,
            max_new_tokens=220,
            do_sample=True,
            temperature=0.25,
            top_p=0.9,
            repetition_penalty=1.15,
            no_repeat_ngram_size=4,
            pad_token_id=tokenizer.pad_token_id,
        )

    full_text = tokenizer.decode(output_ids[0], skip_special_tokens=True)

    if "EVANA:" in full_text:
        reply = full_text.split("EVANA:")[-1].strip()
    else:
        reply = full_text[len(prompt):].strip() or full_text.strip()

    return reply


# @app.post("/chat")
# def chat_endpoint(req: ChatRequest):
#     \"\"\"‡∏à‡∏∏‡∏î‡∏£‡∏±‡∏ö‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡∏à‡∏≤‡∏Å backend (‡∏£‡∏±‡∏ö message ‡πÄ‡∏î‡∏µ‡∏¢‡∏ß‡πÅ‡∏ö‡∏ö‡∏ó‡∏µ‡πà backend ‡∏™‡πà‡∏á‡∏°‡∏≤)\"\"\"
#     answer = generate_answer(req.message)
#     return {"reply": answer}

@app.post("/chat")
def chat_endpoint(req: ChatRequest):
    \"\"\"‡∏à‡∏∏‡∏î‡∏£‡∏±‡∏ö‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡∏à‡∏≤‡∏Å backend (‡∏£‡∏±‡∏ö message ‡πÄ‡∏î‡∏µ‡∏¢‡∏ß‡πÅ‡∏ö‡∏ö‡∏ó‡∏µ‡πà backend ‡∏™‡πà‡∏á‡∏°‡∏≤)\"\"\"
    start = time.time()   # ‡πÄ‡∏£‡∏¥‡πà‡∏°‡∏à‡∏±‡∏ö‡πÄ‡∏ß‡∏•‡∏≤
    answer = generate_answer(req.message)
    latency = time.time() - start
    # üü¶ ‡∏û‡∏¥‡∏°‡∏û‡πå‡πÄ‡∏ß‡∏•‡∏≤‡πÉ‡∏ô console ‡πÅ‡∏ö‡∏ö‡∏™‡∏ß‡∏¢ ‡πÜ
    print(f"[EVANA][Inference] ‡πÉ‡∏ä‡πâ‡πÄ‡∏ß‡∏•‡∏≤ {latency:.3f} ‡∏ß‡∏¥‡∏ô‡∏≤‡∏ó‡∏µ  |  prompt = {req.message[:50]}...")
    return {"reply": answer}

if __name__ == "__main__":
    import uvicorn

    print("üöÄ Starting EVANA inference server on 0.0.0.0:8001 ...")
    uvicorn.run("evana_infer_server_local:app", host="0.0.0.0", port=8001, reload=False)
"""


if __name__ == "__main__":
    import uvicorn

    print("üöÄ Starting EVANA sample inference server on 0.0.0.0:8001 ...")
    uvicorn.run("evana_infer_server_local:app", host="0.0.0.0", port=8001, reload=False)
